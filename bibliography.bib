
@article{schwartz_reinforcement_1993,
	title = {A {Reinforcement} {Learning} {Method} for {Maximizing} {Undiscounted} {Rewards}},
	volume = {Proceedings of the Tenth International Conference},
	url = {https://antonjazz.com/x/grab/AntonSchwartzReinforcementLearningML93.pdf},
	abstract = {While most Reinforcement Learning work uti- lizes temporal discounting to evaluate perfor- mance, the reasons for this are unclear. Is it out of desire or necessity? We argue that it is not out of desire, and seek to dispel the notion that temporal discounting is necessary by proposing a frame- work for undiscounted optimization. We present a metric of undiscounted performance and an al- gorithm for finding action policies that maximize that measure. The technique, which we call R- learning, is modelled after the popular Q-learning algorithm. Initial experimental results are presented which attest to a great improvement over Q-learning in some simple cases.},
	journal = {Machine Learning},
	author = {Schwartz, Anton},
	year = {1993},
	file = {Full Text:/Users/Sayan/Zotero/storage/WAJ7VS8J/Schwartz - 1993 - A Reinforcement Learning Method for Maximizing Und.pdf:application/pdf},
}

@article{fruit_exploration--exploitation_2017,
	title = {Exploration--{Exploitation} in {MDPs} with {Options}},
	url = {http://arxiv.org/abs/1703.08667},
	abstract = {While a large body of empirical results show that temporally-extended actions and options may significantly affect the learning performance of an agent, the theoretical understanding of how and when options can be beneficial in online reinforcement learning is relatively limited. In this paper, we derive an upper and lower bound on the regret of a variant of UCRL using options. While we first analyze the algorithm in the general case of semi-Markov decision processes (SMDPs), we show how these results can be translated to the specific case of MDPs with options and we illustrate simple scenarios in which the regret of learning with options can be {\textbackslash}textit\{provably\} much smaller than the regret suffered when learning with primitive actions.},
	urldate = {2021-08-03},
	journal = {arXiv:1703.08667 [cs, stat]},
	author = {Fruit, Ronan and Lazaric, Alessandro},
	month = apr,
	year = {2017},
	note = {arXiv: 1703.08667},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {fruit17a.pdf:/Users/Sayan/Nextcloud/Thesis/Factored MDP/fruit17a.pdf:application/pdf},
}

@misc{fruit_regret_2017,
	title = {Regret {Minimization} in {MDPs} with {Options} without {Prior} {Knowledge} — supplementary.pdf},
	url = {https://proceedings.neurips.cc/paper/2017/hash/90599c8fdd2f6e7a03ad173e2f535751-Abstract.html},
	abstract = {The option framework integrates temporal abstraction into the reinforcement learning model through the introduction of macro-actions (i.e., options). Recent works leveraged on the mapping of Markov decision processes (MDPs) with options to semi-MDPs (SMDPs) and introduced SMDP-versions of exploration-exploitation algorithms (e.g., RMAX-SMDP and UCRL-SMDP) to analyze the impact of options on the learning performance. Nonetheless, the PAC-SMDP sample complexity of RMAX-SMDP can hardly be translated into equivalent PAC-MDP theoretical guarantees, while UCRL-SMDP requires prior knowledge of the parameters characterizing the distributions of the cumulative reward and duration of each option, which are hardly available in practice. In this paper, we remove this limitation by combining the SMDP view together with the inner Markov structure of options into a novel algorithm whose regret performance matches UCRL-SMDP's up to an additive regret term. We show scenarios where this term is negligible and the advantage of temporal abstraction is preserved. We also report preliminary empirical result supporting the theoretical findings.},
	publisher = {Advances in Neural Information Processing Systems 30},
	author = {Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro and Brunskill, Emma},
	year = {2017},
	file = {Regret Minimization in MDPs with Options without Prior Knowledge — supplementary.pdf:/Users/Sayan/Nextcloud/Thesis/Factored MDP/Regret Minimization in MDPs with Options without Prior Knowledge — supplementary.pdf:application/pdf},
}

@article{mahadevan_average_1996,
	title = {Average reward reinforcement learning: {Foundations}, algorithms, and empirical results},
	abstract = {This paper presents a detailed study of average reward reinforcement learning, an undiscounted optimality framework that is more appropriate for cyclical tasks than the much better studied discounted framework. A wide spectrum of average reward algorithms are described, ranging from synchronous dynamic programming methods to several (provably convergent) asynchronous algorithms from optimal control and learning automata. A general sensitive discount optimality metric called n-discount-optimalityis introduced, and used to compare the various algorithms. The overview identifies a key similarity across several asynchronous algorithms that is crucial to their convergence, namely independent estimation of the average reward and the relative values. The overview also uncovers a surprising limitation shared by the different algorithms: while several algorithms can provably generate gain-optimal policies that maximize average reward, none of them can reliably filter these to produce bias-optimal (or T-optimal) policies that also maximize the finite reward to absorbing goal states. This paper also presents a detailed empirical study of R-learning, an average reward reinforcement learning method, using two empirical testbeds: a stochastic grid world domain and a simulated robot environment. A detailed sensitivity analysis of R-learning is carried out to test its dependence on learning rates and exploration levels. The results suggest that R-learning is quite sensitive to exploration strategies, and can fall into sub-optimal limit cycles. The performance of R-learning is also compared with that of Q-learning, the best studied discounted RL method. Here, the results suggest that R-learning can be fine-tuned to give better performance than Q-learning in both domains.},
	language = {en},
	author = {Mahadevan, Sridhar},
	year = {1996},
	pages = {37},
	file = {Mahadevan - Average reward reinforcement learning Foundations.pdf:/Users/Sayan/Zotero/storage/Z95VFYMF/Mahadevan - Average reward reinforcement learning Foundations.pdf:application/pdf},
}

@misc{silver_lectures_2020,
	title = {Lectures on {Reinforcement} {Learning}},
	url = {https://www.davidsilver.uk/teaching/},
	language = {en-GB},
	urldate = {2021-08-03},
	journal = {Lectures on  Reinforcement Learning},
	author = {Silver, David},
	year = {2020},
	file = {Silver - Lecture 1 Introduction to Reinforcement Learning.pdf:/Users/Sayan/Zotero/storage/JMGTNMT4/Silver - Lecture 1 Introduction to Reinforcement Learning.pdf:application/pdf;Silver - Lecture 2 Markov Decision Processes.pdf:/Users/Sayan/Zotero/storage/KQAQ3RPT/Silver - Lecture 2 Markov Decision Processes.pdf:application/pdf;Snapshot:/Users/Sayan/Zotero/storage/76IMPBPP/teaching.html:text/html},
}

@article{sutton_between_1999,
	title = {Between {MDPs} and semi-{MDPs}: {A} framework for temporal abstraction in reinforcement learning},
	volume = {112},
	issn = {00043702},
	shorttitle = {Between {MDPs} and semi-{MDPs}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370299000521},
	doi = {10.1016/S0004-3702(99)00052-1},
	abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options deﬁned over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macroutility problem. © 1999 Published by Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {1-2},
	urldate = {2021-08-04},
	journal = {Artificial Intelligence},
	author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
	month = aug,
	year = {1999},
	pages = {181--211},
	file = {between_mdps_options_fmdp_sutton.pdf:/Users/Sayan/Nextcloud/Thesis/Factored MDP/between_mdps_options_fmdp_sutton.pdf:application/pdf},
}

@article{jaksch_near-optimal_2010,
	title = {Near-optimal {Regret} {Bounds} for {Reinforcement} {Learning}},
	language = {en},
	author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
	year = {2010},
	pages = {38},
	file = {jaksch10a.pdf:/Users/Sayan/Nextcloud/Thesis/Factored MDP/jaksch10a.pdf:application/pdf;ucrl-jaksch10a.pdf:/Users/Sayan/Nextcloud/Thesis/Factored MDP/ucrl-jaksch10a.pdf:application/pdf},
}

@article{federgruen_denumerable_1983,
	title = {Denumerable {Undiscounted} {Semi}-{Markov} {Decision} {Processes} with {Unbounded} {Rewards}},
	volume = {8},
	issn = {0364-765X, 1526-5471},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/moor.8.2.298},
	doi = {10.1287/moor.8.2.298},
	language = {en},
	number = {2},
	urldate = {2021-09-01},
	journal = {Mathematics of Operations Research},
	author = {Federgruen, A. and Schweitzer, P. J. and Tijms, H. C.},
	month = may,
	year = {1983},
	pages = {298--313},
	file = {Federgruen et al. - 1983 - Denumerable Undiscounted Semi-Markov Decision Proc.pdf:/Users/Sayan/Zotero/storage/BDMAEGX8/Federgruen et al. - 1983 - Denumerable Undiscounted Semi-Markov Decision Proc.pdf:application/pdf},
}

@incollection{puterman_chapter_1990,
	series = {Handbooks in {Operations} {Research} and {Management} {Science}},
	title = {Chapter 8 {Markov} decision processes},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/S0927050705801720},
	abstract = {Publisher Summary This chapter presents theory, applications, and computational methods for Markov Decision Processes (MDP's). MDP's are a class of stochastic sequential decision processes in which the cost and transition functions depend only on the current state of the system and the current action. These models have been applied in a wide range of subject areas, most notably in queueing and inventory control. A sequential decision process is a model for dynamic system under the control of a decision maker. Sequential decision processes are classified according to the times (epochs) at which decisions are made, the length of the decision making horizon, the mathematical properties of the state and action spaces, and the optimality criteria. The focus of this chapter is problems in which decisions are made periodically at discrete time points. The state and action sets are either finite, countable, compact or Borel; their characteristics determine the form of the reward and transition probability functions. The optimality criteria considered in the chapter include finite and infinite horizon expected total reward, infinite horizon expected total discounted reward, and average expected reward. The main objectives in analyzing sequential decision processes in general and MDP's in particular include (1) providing an optimality equation that characterizes the supremal value of the objective function, (2) characterizing the form of an optimal policy if it exists, (3) developing efficient computational procedures for finding policies thatare optimal or close to optimal. The optimality or Bellman equation is the basic entity in MDP theory and almost all existence, characterization, and computational results are based on its analysis.},
	booktitle = {Stochastic {Models}},
	publisher = {Elsevier},
	author = {Puterman, Martin L.},
	year = {1990},
	doi = {https://doi.org/10.1016/S0927-0507(05)80172-0},
	note = {ISSN: 0927-0507},
	pages = {331--434},
}

@article{hordijk_modified_1975,
	title = {A {Modified} {Form} of the {Iterative} {Method} of {Dynamic} {Programming}},
	volume = {3},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/2958088},
	abstract = {This paper considers the discrete time finite state Markovian decision problem with the average return criterion. A modified form of the iterative method of dynamic programming is studied. Under the assumption that the maximal average return is independent of the initial state the asymptotic behaviour of the sequence of functions generated by this modified method is found. It is shown that the modified iterative method supplies both upper and lower bounds on the maximal average return and ε-optimal policies. Moreover, a convergence result is proved for the policies produced by the modified iterative method.},
	number = {1},
	urldate = {2021-09-01},
	journal = {The Annals of Statistics},
	author = {Hordijk, Arie and Tijms, Henk},
	year = {1975},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {203--208},
	file = {JSTOR Full Text PDF:/Users/Sayan/Zotero/storage/6AYMDK5V/Hordijk and Tijms - 1975 - A Modified Form of the Iterative Method of Dynamic.pdf:application/pdf},
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement {Learning}: {An} {Introduction}},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
	keywords = {Reinforcement learning},
	file = {RLbook2020.pdf:/Users/Sayan/Documents/Books/RLbook2020.pdf:application/pdf},
}

@article{kearns_finite-sample_1999,
	title = {Finite-{Sample} {Convergence} {Rates} for {Q}-{Learning} and {Indirect} {Algorithms}},
	volume = {11},
	doi = {https://dl.acm.org/doi/10.5555/340534.340896},
	abstract = {In this paper, we address two issues of long-standing interest in the reinforcement learning literature. First, what kinds of performance guarantees can be made for Q-learning after only a finite number of actions? Second, what quantitative comparisons can be made between Q-learning and model-based (indirect) approaches, which use experience to estimate next-state distributions for off-line value iteration? We first show that both Q-learning and the indirect approach enjoy rather rapid convergence to the optimal policy as a function of the number of state transitions observed. In particular, on the order of only (N log(1=ffl)=ffl 2 )(log(N) + log log(1=ffl)) transitions are sufficient for both algorithms to come within ffl of the optimal policy, in an idealized model that assumes the observed transitions are "well-mixed" throughout an N-state MDP. Thus, the two approaches have roughly the same sample complexity. Perhaps surprisingly, this sample complexity is far less than what is required for the model-based approach to actually construct a good approximation to the next-state distribution. The result also shows that the amount of memory required by the model-based approach is closer to N than to N 2 .},
	journal = {Advances in Neural Information Processing},
	author = {Kearns, Michael and Singh, Satinder},
	month = apr,
	year = {1999},
}

@article{parr_reinforcement_nodate,
	title = {Reinforcement {Learning} with {Hierarchies} of {Machines}},
	abstract = {We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially speciﬁed machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and “behavior-based” or “teleo-reactive” approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.},
	language = {en},
	author = {Parr, Ronald and Russell, Stuart},
	pages = {7},
	file = {Parr and Russell - Reinforcement Learning with Hierarchies of Machine.pdf:/Users/Sayan/Zotero/storage/IFAKYFPH/Parr and Russell - Reinforcement Learning with Hierarchies of Machine.pdf:application/pdf},
}

@inproceedings{bradtke_reinforcement_1995,
	title = {Reinforcement {Learning} {Methods} for {Continuous}-{Time} {Markov} {Decision} {Problems}},
	abstract = {Semi-Markov Decision Problems are continuous time generalizations of discrete time Markov Decision Problems. A number of reinforcement learning algorithms have been developed recently for the solution of Markov Decision Problems, based on the ideas of asynchronous dynamic programming and stochastic approximation. Among these are TD(), Q-learning, and Real-time Dynamic Programming. After reviewing semi-Markov Decision Problems and Bellman's optimality equation in that context, we propose algorithms similar to those named above, adapted to the solution of semi-Markov Decision Problems. We demonstrate these algorithms by applying them to the problem of determining the optimal control for a simple queueing system. We conclude with a discussion of circumstances under which these algorithms may be usefully applied.  1 Introduction  A number of reinforcement learning algorithms based on the ideas of asynchronous dynamic programming and stochastic approximation have been developed recently for...},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Bradtke, Steven and Duff, Michael O.},
	year = {1995},
	pages = {393--400},
	file = {Citeseer - Full Text PDF:/Users/Sayan/Zotero/storage/FQA83GY9/Bradtke and Duff - 1995 - Reinforcement Learning Methods for Continuous-Time.pdf:application/pdf;Citeseer - Snapshot:/Users/Sayan/Zotero/storage/WKIBME6K/summary.html:text/html},
}

@inproceedings{mahadevan_self-improving_1997,
	title = {Self-{Improving} {Factory} {Simulation} using {Continuous}-time {Average}-{Reward} {Reinforcement} {Learning}},
	abstract = {Many factory optimization problems, from inventory control to scheduling and reliability, can be formulated as continuous-time Markov decision processes. A primary goal in such problems is to find a gain-optimal policy that minimizes the long-run average cost. This paper describes a new averagereward algorithm called SMART for finding gain-optimal policies in continuous time semi-Markov decision processes. The paper presents a detailed experimental study of SMART on a large unreliable production inventory problem. SMART outperforms two well-known reliability heuristics from industrial engineering. A key feature of this study is the integration of the reinforcement learning algorithm directly into two commercial discrete-event simulation packages, ARENA and CSIM, paving the way for this approach to be applied to many other factory optimization problems for which there already exist simulation models. 1 Introduction  Many problems in industrial design and manufacturing, such as schedulin...},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann},
	author = {Mahadevan, Sridhar and Marchalleck, Nicholas and Das, Tapas K. and Gosavi, A.},
	year = {1997},
	pages = {202--210},
	file = {Citeseer - Full Text PDF:/Users/Sayan/Zotero/storage/VKEBILRZ/Mahadevan et al. - 1997 - Self-Improving Factory Simulation using Continuous.pdf:application/pdf;Citeseer - Snapshot:/Users/Sayan/Zotero/storage/RGKTM9MW/summary.html:text/html},
}

@inproceedings{jong_utility_2008,
	title = {The utility of temporal abstraction in reinforcement learning},
	abstract = {The hierarchical structure of real-world problems has motivated extensive research into temporal abstractions for reinforcement learning, but precisely how these abstractions allow agents to improve their learning performance is not well understood. This paper investigates the connection between temporal abstraction and an agent’s exploration policy, which determines how the agent’s performance improves over time. Experimental results with standard methods for incorporating temporal abstractions show that these methods benefit learning only in limited contexts. The primary contribution of this paper is a clearer understanding of how hierarchical decompositions interact with reinforcement learning algorithms, with important consequences for the manual design or automatic discovery of action hierarchies.},
	booktitle = {Proceedings of the {Seventh} {International} {Joint} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}},
	author = {Jong, Nicholas K.},
	year = {2008},
	file = {Citeseer - Full Text PDF:/Users/Sayan/Zotero/storage/WJYQ8D53/Jong - 2008 - The utility of temporal abstraction in reinforceme.pdf:application/pdf;Citeseer - Snapshot:/Users/Sayan/Zotero/storage/FPVTW995/summary.html:text/html},
}

@article{brunskill_pac-inspired_2014,
	title = {{PAC}-inspired {Option} {Discovery} in {Lifelong} {Reinforcement} {Learning}},
	abstract = {A key goal of AI is to create lifelong learning agents that can leverage prior experience to improve performance on later tasks. In reinforcement-learning problems, one way to summarize prior experience for future use is through options, which are temporally extended actions (subpolicies) for how to behave. Options can then be used to potentially accelerate learning in new reinforcement learning tasks. In this work, we provide the ﬁrst formal analysis of the sample complexity, a measure of learning speed, of reinforcement learning with options. This analysis helps shed light on some interesting prior empirical results on when and how options may accelerate learning. We then quantify the beneﬁt of options in reducing sample complexity of a lifelong learning agent. Finally, the new theoretical insights inspire a novel option-discovery algorithm that aims at minimizing overall sample complexity in lifelong reinforcement learning.},
	language = {en},
	journal = {In Proceedings of the 31st International Conference on Machine Learning},
	author = {Brunskill, Emma and Li, Lihong},
	year = {2014},
	pages = {9},
	file = {Brunskill and Li - PAC-inspired Option Discovery in Lifelong Reinforc.pdf:/Users/Sayan/Zotero/storage/TFP3GA29/Brunskill and Li - PAC-inspired Option Discovery in Lifelong Reinforc.pdf:application/pdf},
}

@inproceedings{talebi_variance-aware_2018,
	title = {Variance-{Aware} {Regret} {Bounds} for {Undiscounted} {Reinforcement} {Learning} in {MDPs}},
	url = {https://proceedings.mlr.press/v83/talebi18a.html},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {Algorithmic {Learning} {Theory}},
	publisher = {PMLR},
	author = {Talebi, Mohammad Sadegh and Maillard, Odalric-Ambrym},
	month = apr,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {770--805},
	file = {Full Text PDF:/Users/Sayan/Zotero/storage/P9ZESTKJ/Talebi and Maillard - 2018 - Variance-Aware Regret Bounds for Undiscounted Rein.pdf:application/pdf},
}
