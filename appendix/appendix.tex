
\appendix

\chapter{MDP Implementation}

A class to represent Markov Decision Processes is implemented as follows. A thing of note with this class is the state property that is shared with the class that inherits it.

\begin{minted}[fontsize=\footnotesize, breaklines]{python3}
"""
MDP base class.
"""
class MDP:
    def __init__(self, n_states, n_actions):
        self.n_states = n_states
        self.n_actions = n_actions

        self.__state = None

    @property
    def state(self):
        return self.__state

    @state.setter
    def state(self, state):
        # print(f"Setting state to {state}")
        assert 0 <= state < self.n_states, f"0 </= {state} </ {self.n_states}"

        if 0 <= state < self.n_states:
            self.__state = state
            # print(f"Set state to {self.__state}")
\end{minted}

\chapter{SMDP Implementation}

Semi-Markov Decision Processes inherit from the MDP class, they are implemented as follows

\begin{minted}[fontsize=\footnotesize, breaklines]{python3}
from collections import defaultdict
import numpy as np
from ConfidenceBounds import EmpBernsteinPeeling
from MDP import MDP
from tqdm import tqdm


class SemiMDP(MDP):
    def __init__(self, n_states, n_actions, options, r_max):
        super().__init__(n_states, n_actions)

        self.n_states = n_states
        self.n_actions = n_actions

        self.n_options = len(options)
        self.options = options
        self.transition_probabilities = None

        self.target = None
        self.r_max = r_max

        # Visitation counts of S x O.
        self.N = np.zeros((n_states, self.n_options))

        # Transition counts of S x O x S.
        self.P = np.zeros((n_states, self.n_options, n_states))

        # Accumulated rewards S x O.
        self.R = np.zeros((n_states, self.n_options))

        # Options lookup table.
        self.options_lut = defaultdict(list)

        for o in options:
            for i in o.initiation_states:
                self.options_lut[i].append(o.o_index)

    def get_next_states(self, state, option):
        next_states = []

        for s, p in enumerate(self.transition_probabilities[option][state]):
            if p > 0:
                next_states.append((s, p))

        return next_states

    def state_option_to_index(self, state, option):
        return np.ravel_multi_index((state, option), (self.n_states, self.n_options))

    def index_to_state_option(self, index):
        return np.unravel_index(index, (self.n_states, self.n_options))

    def has_experience_doubled(self, previous_experience):
        has_doubled = False

        for s in range(self.n_states):
            for o in range(self.n_options):
                if self.N[s][o] > 2 * previous_experience[s][o]:
                    has_doubled = True

        return has_doubled

    def estimate_transition_probability(self, alpha=0.1):
        p_hat = np.sum(self.P, axis=1)

        # for s in range(self.n_states):
        #     for o in range(self.n_options):
        #         for s_prime in range(self.n_states):
        #             p_hat[s][s_prime] += self.P[s][o][s_prime]

        for s in range(self.n_states):
            # Smooth estimate.
            row_sum = np.sum(p_hat[s])
            p_hat[s] = (p_hat[s] + alpha)/max(1, (row_sum + alpha * self.n_states))

        return p_hat

    def estimate_transition_bound(self, delta=0.05, t=1):
        lbs, ubs = [], []

        for o in range(self.n_options):
            p = np.zeros((self.n_states, self.n_states))
            n_plus = np.zeros((self.n_states))
            n = np.zeros((self.n_states))

            for s in range(self.n_states):
                for s_prime in range(self.n_states):
                    p[s][s_prime] += self.P[s][o][s_prime]
                    n[s] += self.P[s][o][s_prime]

            for s in range(self.n_states):
                p[s] /= max(1, np.sum(p[s]))
                n_plus[s] = max(1, n[s])


            # Using EmpBernsteinPeeling.
            CB = EmpBernsteinPeeling()
            lb, ub = CB.confidencebound(self.n_states,
                                        p,
                                        n_plus,
                                        n,
                                        delta,
                                        t)

            # assert lb.shape == p.shape, f"{lb.shape} =/= {p.shape}"
            # assert ub.shape == p.shape, f"{ub.shape} =/= {p.shape}"

            lbs.append(lb)
            ubs.append(ub)

        return np.array(lbs), np.array(ubs)

    def compute_true_transition_probability(self):
        P = np.zeros((self.n_options, self.n_states, self.n_states))

        for no, o in enumerate(self.options):
            for s in range(self.n_states):
                if s in o.initiation_states:
                    N = len(o.termination_condition)
                    states, termination_probs = zip(*o.termination_condition)
                    probs = []

                    for i in range(N):
                        p = termination_probs[i]
                        for j in range(i):
                            p *= (1 - termination_probs[j])

                        probs.append(p)

                    # assert np.isclose(sum(probs), 1.0), f"Sum of probabilities should be 1; but isn't!"

                    for i, s_prime in enumerate(states):
                        P[no][s][s_prime] = probs[i]

        return P

    def compute_true_reward(self):
        R = np.zeros((self.n_states, self.n_options))

        for s in range(self.n_states):
            if s == self.target:
                for no in self.get_options(s):
                    R[s][no] = self.r_max

        return R

    def solve(self, true_rewards):
        v_old = np.zeros(self.n_states)
        v_diff = np.arange(self.n_states)
        pi = np.zeros(self.n_states)

        iter = 0

        while iter < 1000 and max(v_diff) - min(v_diff) >= 10**-6:
            iter += 1

            v = np.zeros(self.n_states)

            for s in range(self.n_states):
                v_max_over_options = 0
                maximizing_option = None

                # Max over options.
                for o in self.get_options(s):
                    q, inner_states = self.options[o].trueP0(self)

                    v_tmp = 0
                    mu_hat = self.options[o].estimate_stationary_distribution(q)

                    # Handle terminal option.
                    if s == self.target:
                        b_o = np.ones(self.n_states)
                        b_o /= np.sum(b_o)
                    else:
                        b_o = self.compute_true_transition_probability()[o][s]

                    for s_prime_idx, s_prime in enumerate(inner_states):
                        v_tmp += true_rewards[s_prime][o] * mu_hat[s_prime_idx]

                    mu_hat_padded = np.zeros(self.n_states)

                    for s_idx, state in enumerate(inner_states):
                        mu_hat_padded[state] = mu_hat[s_idx]

                    v_tmp += mu_hat_padded[s] * (np.dot(b_o.T, v_old) - v_old[s]) + v_old[s]

                    if v_tmp >= v_max_over_options:
                        v_max_over_options = v_tmp
                        maximizing_option = o

                v[s] = v_max_over_options
                pi[s] = maximizing_option

            # print(f"{v = }, {self.target = }")

            v_diff = v - v_old
            v_old = v

        gain = v_diff[0]

        return pi, gain

    def act(self, option_id):
        raise NotImplementedError("Implement in inheriting class.")

    def get_options(self, state=None):
        state = self.state if state is None else state
        return self.options_lut[state]
\end{minted}

\chapter{Grid World Domain Implementation}

The grid world domain is implemented as follows.

\begin{minted}[fontsize=\footnotesize, breaklines]{python3}
import numpy as np
from copy import deepcopy as dc
from SemiMDP import SemiMDP
from Option import Option

class GridWorldOptions(SemiMDP):
    action_lut = {
        "U": 0,
        "D": 1,
        "L": 2,
        "R": 3,
        "X": 4
    }

    option_lut = action_lut

    def __init__(self, d, r_max=1.0, teleport_on_reaching_goal=True, m=2):
        self.d = d
        self.r = None
        self.c = None

        self.teleport_on_reaching_goal = teleport_on_reaching_goal

        self.n_states = d * d               # Total number of positions on the grid is d^2.
        self.n_actions = 4                  # 4 actions - U,D,L,R.
        self.n_options = 4 * self.n_states  # Each state has 4 options - U,D,L,R.

        self.m = m
        self.r_max = r_max

        self.option_counter = 0

        options = []

        self.has_terminated = False

        self.r = np.random.randint(self.d)
        self.c = np.random.randint(self.d)

        state = self.row_col_to_state(self.r, self.c)
        target = np.random.randint(self.n_states)

        for rr in range(self.d):
            for cc in range(self.d):
                if target == self.row_col_to_state(rr, cc):
                    # Target state has option to trap.
                    options.append(self.__get_terminal_state_option(rr, cc))
                else:
                    # All other states have other options.
                    options.append(self.__get_up_option(rr, cc))
                    options.append(self.__get_down_option(rr, cc))
                    options.append(self.__get_right_option(rr, cc))
                    options.append(self.__get_left_option(rr, cc))

        super().__init__(self.n_states, self.n_actions, options, r_max)

        self.transition_probabilities = self.compute_true_transition_probability()
        self.state = state
        self.target = target


    def __get_up_state(self, row, col):
        new_r, new_c = min(row + 1, self.d - 1), col
        return self.row_col_to_state(new_r, new_c)

    def __get_down_state(self, row, col):
        new_r, new_c = max(row - 1, 0), col
        return self.row_col_to_state(new_r, new_c)

    def __get_right_state(self, row, col):
        new_r, new_c = row, min(col + 1, self.d - 1)
        return self.row_col_to_state(new_r, new_c)

    def __get_left_state(self, row, col):
        new_r, new_c = row, max(col - 1, 0)
        return self.row_col_to_state(new_r, new_c)

    def __get_terminal_state_option(self, row, col):
        init_state = self.row_col_to_state(row, col)
        next_states = [(init_state, 1.0)]

        option = Option(n_states=self.n_states,
                        n_actions=1,
                        initiation_states=[init_state],
                        termination_condition=next_states,
                        policy=[self.option_counter] * self.n_states,
                        reward=self.r_max,
                        name="X",
                        o_index=self.option_counter)

        self.option_counter += 1

        return option

    def __get_up_option(self, row, col):
        next_states = [(x, col) for x in range(row + 1, min(self.d, row + self.m + 1))]
        # if len(next_states) == 0: # Hit top wall
        #     next_states = [(row, col)]
        next_states = [(row, col)] + next_states

        t_states = list(map(lambda tuple: self.row_col_to_state(tuple[0], tuple[1]),
                            next_states))

        # print(f"U({row, col} [{self.row_col_to_state(row, col)}]) -> {list(zip(next_states, t_states))}")

        t_probs = np.zeros(len(t_states))
        for k in range(len(t_probs)):
            t_probs[k] = 1/(len(t_probs) - k)

        # print(f"{t_probs = }")

        init_state = self.row_col_to_state(row, col)

        option = Option(n_states=self.n_states,
                        n_actions=1,
                        initiation_states=[init_state],
                        termination_condition=list(zip(t_states, t_probs)),
                        policy=[self.option_counter] * self.n_states,
                        reward=0,
                        name="U",
                        o_index=self.option_counter)

        self.option_counter += 1

        return option

    def __get_down_option(self, row, col):
        next_states = [(x, col) for x in range(max(0, row - self.m - 1), row)]
        # if len(next_states) == 0: # Hit bottom wall
        #     next_states = [(row, col)]
        next_states = [(row, col)] + next_states

        t_states = list(map(lambda tuple: self.row_col_to_state(tuple[0], tuple[1]),
                            next_states))

        # print(f"D({row, col} [{self.row_col_to_state(row, col)}]) -> {list(zip(next_states, t_states))}")

        t_probs = np.zeros(len(t_states))
        for k in range(len(t_probs)):
            t_probs[k] = 1/(len(t_probs) - k)

        # print(f"{t_probs = }")

        init_state = self.row_col_to_state(row, col)

        option = Option(n_states=self.n_states,
                        n_actions=1,
                        initiation_states=[init_state],
                        termination_condition=list(zip(t_states, t_probs)),
                        policy=[self.option_counter] * self.n_states,
                        reward=0,
                        name="D",
                        o_index=self.option_counter)

        self.option_counter += 1

        return option

    def __get_right_option(self, row, col):
        next_states = [(row, x) for x in range(col + 1, min(self.d, col + self.m + 1))]
        # if len(next_states) == 0: # Hit right wall
        #     next_states = [(row, col)]
        next_states = [(row, col)] + next_states

        t_states = list(map(lambda tuple: self.row_col_to_state(tuple[0], tuple[1]),
                            next_states))

        # print(f"R({row, col} [{self.row_col_to_state(row, col)}]) -> {list(zip(next_states, t_states))}")

        t_probs = np.zeros(len(t_states))
        for k in range(len(t_probs)):
            t_probs[k] = 1/(len(t_probs) - k)

        # print(f"{t_probs = }")

        init_state = self.row_col_to_state(row, col)

        option = Option(n_states=self.n_states,
                        n_actions=1,
                        initiation_states=[init_state],
                        termination_condition=list(zip(t_states, t_probs)),
                        policy=[self.option_counter] * self.n_states,
                        reward=0,
                        name="R",
                        o_index=self.option_counter)

        self.option_counter += 1

        return option

    def __get_left_option(self, row, col):
        next_states = [(row, x) for x in range(max(0, col - self.m - 1), col)]
        # if len(next_states) == 0: # Hit left wall
        #     next_states = [(row, col)]
        next_states = [(row, col)] + next_states

        t_states = list(map(lambda tuple: self.row_col_to_state(tuple[0], tuple[1]),
                            next_states))

        # print(f"L({row, col} [{self.row_col_to_state(row, col)}]) -> {list(zip(next_states, t_states))}")

        t_probs = np.zeros(len(t_states))
        for k in range(len(t_probs)):
            t_probs[k] = 1/(len(t_probs) - k)

        # print(f"{t_probs = }")

        init_state = self.row_col_to_state(row, col)

        option = Option(n_states=self.n_states,
                        n_actions=1,
                        initiation_states=[init_state],
                        termination_condition=list(zip(t_states, t_probs)),
                        policy=[self.option_counter] * self.n_states,
                        reward=0,
                        name="L",
                        o_index=self.option_counter)

        self.option_counter += 1

        return option

    def row_col_to_state(self, row, col):
        return np.ravel_multi_index((row, col), (self.d, self.d))

    def state_to_row_col(self, state):
        return np.unravel_index(state, (self.d, self.d))

    def get_state(self):
        return self.row_col_to_state(self.r, self.c)

    def act(self, option_id):
        option = self.options[option_id]
        t_state = dc(self.state)
        reward = 0

        if t_state == self.target:
            # print("In goal state.")
            reward = self.r_max

        if option.is_valid(t_state):
            # print(f"Acting with option: {option.name}")
            next_state, _, holding_time = option.act(self)
            self.state = next_state

            self.R[t_state][option_id] += reward
            self.P[t_state][option_id][next_state] += 1
            self.N[t_state][option_id] += 1

            self.r, self.c = self.state_to_row_col(next_state)
            self.state = self.row_col_to_state(self.r, self.c)

            return next_state, reward, holding_time
        else:
            # Sanity check. This should never happen.
            raise ValueError(f"Can't initiate {option} from state `{t_state}`")

    def teleport(self):
        self.r = np.random.randint(self.d)
        self.c = np.random.randint(self.d)
        self.state = self.row_col_to_state(self.r, self.c)

    def has_terminated(self):
        return self.has_terminated

    def __repr__(self):
        to_print = f"GridWorldOptions({self.n_states = }, {self.n_options = }, {self.d = }, {self.state = }, {self.target = })\n"

        return to_print

if __name__ == "__main__":
    env = GridWorldOptions(3)
    print(env)
\end{minted}

\newpage
