\chapter{Learning Algorithms}

In this chapter, the UCRL2 algorithm along with Extended Value Iteration is presented briefly.
Then, its extension to the options framework is described.

\section{UCRL2}

The UCRL2 algorithm was introduced by Jaksch et al. \cite{jaksch_near-optimal_2010}.
It builds on the principle of optimism in the face of uncertainty; which chooses actions assuming that the environment is as nice as possible.
This works because if the optimism is justified then the agent was acting optimally, and, if the optimism is not justified, then the agent gains a better understanding of the environment as it gains more experience.

From a set of MDPs $\mathcal{M}$ constructed from observed statistics of an environment through interaction, the algorithm chooses an MDP $M$ such that it maximizes the gain.
It then computes a policy $\pi$ that is optimal to this optimistic MDP $M$.

The UCRL2 algorithm is episodic in nature, the optimal policy is computed at the beginning of each episode through Extended Value Iteration (EVI).
The episodes in the UCRL2 algorithm do not correspond to the episodes in the environment, instead they are characterised by the execution of the same policy during the episode.
The length of the episodes are not fixed and depend on the observations collected from the environment.
UCRL2 computes estimates for the average reward and the transition probability from the observations made before the $k$-th episode.
Throughout the episode, actions are chosen by the policy computed using EVI on the optimistic MDP from a set of MDPs.
An episode terminates whenever an action is chosen as many times as it was previously chosen in a state $s$, in other words, when the experience of the agent has doubled.
These visitation counts are recorded locally by $v_k(s, a)$ for each pair of state and action in the $k$-th episode.
At the end of each episode $k$, the global visitation counts $N_k(s,a)$ are updated with the values stored in $v_k(s, a)$ for each pair of state and action.

Let the empirical estimate for the rewards at end of episode $k$ be defined as  
$$\hat{\mu}_k(s, a) = \frac{\sum_{t=1}^k R(S_t, A_t) \mathbbm{1}_{S_t = s, A_t = a}}{\max\left\{ 1, N_k(s,a) \right\}}$$
where, $R(S_t, A_t)$ is the reward realized in state $S_t$ by taking action $A_t$.

Let the empirical estimate for the state transition probabilities at end of episode $k$ be defined as  
$$\hat{p}_k(s^\prime \mid s, a) = \frac{N_k(s, a, s^\prime)}{\max\left\{ 1, N_k(s,a) \right\}}$$
where, $N_k(s, a, s^\prime)$ is the number of visits to state $s$, taking action $a$ followed by a visit to state $s^\prime$ till the $k$-th episode.

Formally, the set of MDPs $\mathcal{M}$ maintained by UCRL2 is defined as
$$\mathcal{M} = \{ M(\mathcal{S}, \mathcal{A}, \tilde{R}, \tilde{p}) : \tilde{p}(\cdot \mid s, a) \in c(s, a), \tilde{\mu}(s, a) \in c^\prime(s, a)\}$$
where, $c$ and $c^\prime$ are high probability confidence sets for the average reward $\hat{\mu}$ (mean of $\hat{R}$) and the transition probability $\hat{p}$ respectively.

For a specific value of the confidence parameter $\delta \in (0,1)$, the confidence sets $c$ and $c^\prime$ at time step $t$ are defined as follows
\begin{equation}
c_{\delta, t} (s,a) = \left\{ q \in \Delta(\mathcal{S}): \| \hat{p}_t (\cdot \mid s, a) - q \|_1 \le \sqrt{ \frac{14S}{N_t(s,a)}\log\left( \frac{2At}{\delta} \right)}\right\}
\label{eq:ci_p}
\end{equation}
and,
\begin{equation}
c^\prime_{\delta, t} (s,a) = \left\{ \lambda \in [0,1]: | \hat{\mu}_t (s, a) - \lambda | \le \sqrt{ \frac{7}{2N_t(s,a)}\log\left( \frac{2SAt}{\delta} \right)}\right\}
\label{eq:ci_r}
\end{equation}
where, $S$ is the cardinality of state space $\mathcal{S}$ and $A$ is the cardinality of action space $\mathcal{A}$.

The pseudo-code for the UCRL2 procedure is described in Algorithm \ref{alg:ucrl2}.
\begin{algorithm}[!htbp]
\caption{UCRL2}\label{alg:ucrl2}

\SetKwInput{KwInit}{Initialize}

\KwIn{State space $\mathcal{S}$, action space $\mathcal{A}$ and confidence parameter $\delta \in (0, 1)$}
\KwInit{$N_0(s,a) = 0$ and $v_0(s, a) = 0$ for all $(s, a) \in \mathcal{S} \times \mathcal{A}$, Set $t = 0$ and $k = 1$}
\BlankLine
\For{episode $k = 1, 2, \dots$}{
    \BlankLine
    \tcp{Update global visitation counts using local visitation counts from previous episode.}
    Set $N_k(s, a) = N_{k-1}(s, a) + v_{k-1}(s, a)$ for all $(s, a)$
    
    \BlankLine
    \tcp{Obtain new estimates and compute new policy for $k$-th episode.}
    Estimate $\hat{p}(\cdot \mid s, a)$ and $\hat{\mu}(s, a)$ empirically for all $(s, a)$
    
    Compute $\pi_k$ using extended value iteration with confidence $\delta$
    
    \BlankLine
    Set $v_k(s, a) = 0$ for all $(s, a)$
    
    \BlankLine
    \tcp{Choose actions with $\pi_k$ till experience of any state-action pair doubles.}
    \While{$v_k (s, \pi_k(s)) < \max{\{ 1, N_k(s, \pi_k(s) \}} $}{
        \BlankLine
        \tcp{Observe, act and receive reward.}
        Observe state $s_t$
        
        Execute action $a_t \sim \pi_k(\cdot \mid s_t)$
        
        Receive reward $R(s_t, a_t)$
        
        \BlankLine
        \tcp{Update local visitation count for the current episode and increment the time step.}
        Set $v_k(s_t, a_t) = v_k(s_t, a_t) + 1$
        
        Set $t = t + 1$
        \BlankLine
    }
}
\end{algorithm}

For a confidence parameter $\delta \in (0, 1)$, the regret under UCRL2 in any communicating MDP with $S$ states, $A$ actions and diameter $D$ is

\begin{equation*}
    \Delta(T) = 34DS\sqrt{AT\log{T/\delta}}
\end{equation*}

with probability at least $1 - \delta$ and uniformly for all $T \ge 2$. The \textbf{diameter} $D$ is defined as $D \doteq \max_{s \ne s^\prime} \min_\pi \mathbb{E} \left[ T^\pi(s, s^\prime) \right]$ where $T^\pi(s, s^\prime)$ is the the time to reach state $s^prime$ starting from the initial state $s$ following the stationary policy $\pi$.

\section{Extended Value Iteration}

UCRL2 necessitates the computation of a near-optimal policy for an optimistic MDP $M_k$ during the $k$-th episode.
While value iteration is able to compute an optimal policy for a single MDP, the UCRL2 algorithm requires computing a near-optimal policy in addition to selecting an optimistic MDP $M_k$ that maximizes the average reward.
Extended Value Iteration (EVI) is used to that end.

Let the state value of state $s$ at $k$-th iteration be denoted by $u_k(s)$ and let $u_0(s) = 0$ for all states, then,
\begin{equation}
    u_k(s) = \max_{a \in \mathcal{A}} \left( \tilde{\mu}(s,a) + \sum_{s^\prime \in \mathcal{S}} {\tilde{p}(s^\prime \mid s, a) u_{k-1}(s^\prime)} \right)
\end{equation}
where, 
$$\tilde{\mu}(s,a) \in \max \{ \lambda \in c^\prime_{\delta, k} (s,a) \}$$
from equation \ref{eq:ci_r} and 
$$\tilde{p}(\cdot \mid s, a) \in \arg\max \{ \sum_{s^\prime \in \mathcal{S}} {q(s^\prime) u_k(s^\prime) : q \in c_{\delta, k} (s,a)} \}$$ 
from equation  \ref{eq:ci_p}.
The pseudo-code for the extended value iteration procedure is described in Algorithm \ref{alg:evi}.

\begin{algorithm}
\caption{Extended Value Iteration}\label{alg:evi}

\SetKwInput{KwInit}{Initialize}
\SetKwInput{KwOut}{Output}

\KwIn{Set of MDPs $\mathcal{M}$, confidence parameter $\delta \in (0, 1)$}
\KwInit{$u_0 \equiv 0 $, $u_{-1} \equiv -\infty$ and $k = 0$}

\BlankLine
\While{$\max_s {u_k (s) - u_{k-1} (s)} - \min_s {u_k (s) - u_{k-1} (s)} > \delta$}{
    \BlankLine
    For all state and action pairs $(s, a)$
    \BlankLine
    Compute $\mu^\prime(s,a) \in \max \{ \lambda \in c^\prime_{\delta, k} (s,a) \}$

    Compute $p^\prime(\cdot \mid s, a) \in \arg\max \{ \sum_{s^\prime \in \mathcal{S}} {q(s^\prime) u_k(s^\prime) : q \in c_{\delta, k} (s,a)} \}$
    \BlankLine
    For all states $s \in \mathcal{S}$, update
    \BlankLine
    $u_k(s) = \max_{a \in \mathcal{A}} \left( \mu^\prime(s,a) + \sum_{s^\prime \in \mathcal{S}} {p^\prime(s^\prime \mid s, a) u_{k-1}(s^\prime)} \right)$
    
    $\pi_k(s) = \arg\max_{a \in \mathcal{A}} \left( \mu^\prime(s,a) + \sum_{s^\prime \in \mathcal{S}} {p^\prime(s^\prime \mid s, a) u_{k-1}(s^\prime)} \right)$
    \BlankLine
    
    Increment $k$ to $k+1$.
    \BlankLine
}
\BlankLine
\KwOut{$\pi_{k+1}$}
\end{algorithm}

\section{Extending to MDP with Options}

Fruit et al. \cite{fruit_exploration--exploitation_2017, fruit_regret_2017} combine the SMDP view on options along with the intrinsic MDP structure underlying their execution to achieve temporal abstraction without relying on typically unknown parameters such as the transition probabilities and rewards involved.
Each option is mapped to its associated irreducible Markov chain and optimistic policies are computed using only the stationary distribution of these irreducible Markov chains and the state-transition dynamics induced by the SMDP. 
This approach works without explicit estimates of cumulative rewards, holding times (Section \ref{sec:holding_time}) and their confidence intervals.

\begin{assumption}
\label{asm:option}
All options in the SMDP terminate in finite time, in all terminal states of the option, there is at least one option that can initiate and the resulting SMDP is communicating.
\end{assumption}
Lemma 3 of \cite{fruit_exploration--exploitation_2017} shows that under these assumptions, the family of SMDPs induced by extending MDPs with options is such that for each of the options, the distributions of the cumulative reward and the duration of options are sub-exponential\footnote{A mean-zero random variable $X$ is ($\tau^2, b$) sub-exponential iff $\mathbb{E}[\exp(\lambda X)] \le \exp\left( \frac{\lambda^2 \tau^2}{2} \right)$ for $|\lambda| \le \frac{1}{b}$}.

For a SMDP $\langle \mathcal{S}, \mathcal{O}, \mathcal{P}, \mathcal{R} \rangle$, the optimal gain is formally defined as 

\begin{equation}
    \rho^* \overset{\mathrm{def}}{=} \max_\pi \rho^\pi = \max_\pi \lim_{t \to \infty} \mathbb{E}^\pi \left[ \frac{\sum{R_i}}{t} \right]
\end{equation}

where $R_i$ is the reward accumulated by an option executed at time step $i$. Alternately, the optimal gain may also be expressed as

\begin{equation}
    \label{eq:optimal_bias}
    \rho^* = \max_{o \in \mathcal{O}_s} \left\{ \frac{\bar{R}(s, o)}{\bar{\tau}(s, o)} + \frac{1}{\bar{\tau}(s, o)} \sum_{s^\prime \in \mathcal{S}} p_o(s^\prime \mid s, o) u^*(s^\prime) - u^*(s) \right\}
\end{equation}

by performing a data transformation as defined in Lemma 2 of \cite{federgruen_denumerable_1983}. $u^*$ is the optimal bias and $\mathcal{O}_s$ is the set of options that can be initiated from state $s$ in the above equation.

The performance of a learning algorithm is evaluated through a measure of regret as described in definition \ref{def:regret}.

\begin{definition}
\label{def:regret}
For a SMDP $M$, starting from any state $s \in \mathcal{S}$, and proceeding for $k \ge 1$ decision steps, let $\{\tau_i\}_{i=1}^k$ be the random holding times observed by acting according to policy learned by the learning algorithm $\mathcal{A}$, then the total regret is defined as
\begin{equation}
    \Delta(M, \mathcal{A}, s, k) = \left( \sum_{i=1}^k \tau_i \right) \rho^*(M) - \sum_{i=1}^k r_i
\end{equation}
where, $r_i$ is the reward obtained at the $i$-th decision step and $\rho^*$ is the average asymptotic reward.
\end{definition}

Definition \ref{def:regret} reduces to the standard definition of regret in a MDP when $\tau_i = 1$, that is when all options are primitive actions.
The regret measures the difference in the cumulative reward that can be achieved by the following the optimal policy and the cumulative reward that is actually obtained by the policy learned by the learning algorithm $\mathcal{A}$.
As the total duration of an option after $i$ decision steps varies on the learned policy, even though the average asymptotic reward is characterised by $\rho^*$, the two quantities are multiplied to obtain the true performance of the optimal policy $\pi^*$ after $i$ decision steps.

