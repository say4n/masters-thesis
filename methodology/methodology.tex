\chapter{Methodology}

In this chapter, the methodology for implementation of the learning algorithms and its various constituents are described.
In addition to it, confidence bounds are delved into briefly.

\section{Smooth estimate of Transition Probability}

As the process of interaction of the agent with the environment is inherently noisy, a smoothing of the transition probability estimates is applied to aid learning.
The smooth probability estimate is defined as

\begin{equation}
    P_o(s, s^\prime) = \frac{P_o(s, s^\prime) + \alpha}{\sum_{x \in \mathcal{S}} \left(P_o(s, x) + \alpha \right)}
\end{equation}

where, $\alpha$ is the smoothing factor.

\section{Stationary Distribution of Markov Chain}

The stationary distribution of the Markov chain describes the distribution of the states after a sufficiently long interval of time such that the distribution is not time varying.
If $\mu$ is a column vector representing the probabilities of the states that the Markov chain can visit, then $\mu$ is the stationary distribution of the Markov chain if it satisfies 

\begin{equation}
    \label{eqn:mu_estimate}
    \mu^\top = \mu^\top P
\end{equation}


where $P$ is the transition probability matrix of the Markov chain. 
In other words, $\mu$ is the fixed point of the above equation.
Transition probabilities defining ergodic Markov chains are guaranteed to have a stationary distribution of states.

From equation \ref{eqn:mu_estimate}, it can be seen that $\mu$ is the left eigenvector of $P$ with eigenvalue 1.
Thus, $\mu$ can be computed by eigendecomposition of the transition probability matrix $P$. 
The eigenvector whose eigenvalue is 1 can be suitably normalized to obtain the stationary distribution of the Markov chain.

\section{Confidence Bounds}

The confidence bounds for $\mu_o$ may be calculated either implicitly or explicitly.
Fruit et al. \cite{fruit_regret_2017} use Hoeffding and empirical Bernstein type confidence intervals for the SMDP transition probabilities, the Markov chain associated with each option and the rewards.
In this work, we have used empirical Bernstein type confidence intervals to obtain upper bounds on the estimates.


\section{Putting it all together}

At the start of each experiment, a new environment is instantiated and the various statistics are reset.
A new policy is evaluated only once experience has doubled for at least one of the state-option pairs.
Post completion of the simulations, the plots for accumulated regret as well as accumulated reward are generated.
The experimentation performed as well as the results obtained are described in the following chapter.
